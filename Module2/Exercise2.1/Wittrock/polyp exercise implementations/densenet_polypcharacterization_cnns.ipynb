{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4754d-5e52-44e9-ab38-d7b720c97b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a879d0d3-058f-4e7c-9a74-778e4e721852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Aug  1 19:42:57 2024\n",
    "\n",
    "@author: vigo\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import PIL\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, resnet50, densenet121, efficientnet_b0\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Hyper-parameters \n",
    "batch_size = 32\n",
    "n_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "\n",
    "# Specify the root folders for neoplastic and nonneoplastic images\n",
    "neoplastic_folder = './PCData/Neoplasia'\n",
    "nonneoplastic_folder = './PCData/Non-neoplasia'  \n",
    "\n",
    "\n",
    "nonneoplastic_data = [(os.path.join(nonneoplastic_folder, file), 0) for file in os.listdir(nonneoplastic_folder) if not file.startswith('.')]\n",
    "neoplastic_data = [(os.path.join(neoplastic_folder, file), 1) for file in os.listdir(neoplastic_folder) if not file.startswith('.')]\n",
    "data = neoplastic_data + nonneoplastic_data\n",
    "\n",
    "# Create the train dataset using train_data.txt\n",
    "train_data = []\n",
    "train_ids = set()  # Define train_ids before adding elements to it\n",
    "# Assuming train_data.txt contains data in the format: id \\t ccenum \\t class_type\n",
    "with open('train_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Assuming train_data.txt contains data in the format: id \\t ccenum \\t class_type\n",
    "        sdkid = line.strip().split(',')[0]\n",
    "        train_ids.add(sdkid)\n",
    "\n",
    "\n",
    "# Iterate through your neoplastic and nonneoplastic data\n",
    "for filepath, label in data:\n",
    "    filename = os.path.basename(filepath)\n",
    "    sdkid_train = filename.split('_')[0]  # Assuming sdkid is the prefix before '_'\n",
    "    #print(sdkid_data)\n",
    "    if sdkid_train in train_ids:\n",
    "        train_data.append((filepath, label))\n",
    "\n",
    "\n",
    "\n",
    "# Create the valid dataset using valid_data.txt\n",
    "valid_data = []\n",
    "valid_ids = set()  # Define train_ids before adding elements to it\n",
    "# Assuming train_data.txt contains data in the format: id \\t ccenum \\t class_type\n",
    "with open('valid_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Assuming train_data.txt contains data in the format: id \\t ccenum \\t class_type\n",
    "        sdkid = line.strip().split(',')[0]\n",
    "        valid_ids.add(sdkid)\n",
    "\n",
    "# Iterate through your neoplastic and nonneoplastic data\n",
    "for filepath, label in data:\n",
    "    filename = os.path.basename(filepath)\n",
    "    sdkid_val = filename.split('_')[0]  # Assuming sdkid is the prefix before '_'\n",
    "    #print(sdkid_data)\n",
    "    if sdkid_val in valid_ids:\n",
    "        valid_data.append((filepath, label))\n",
    "              \n",
    "# Create the test dataset using train_data.txt\n",
    "test_data = []\n",
    "test_ids = set()  # Define train_ids before adding elements to it\n",
    "# Assuming train_data.txt contains data in the format: id \\t ccenum \\t class_type\n",
    "with open('test_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Assuming train_data.txt contains data in the format: id \\t ccenum \\t class_type\n",
    "        sdkid = line.strip().split(',')[0]\n",
    "        test_ids.add(sdkid)\n",
    "\n",
    "# Iterate through your neoplastic and nonneoplastic data\n",
    "for filepath, label in data:\n",
    "    filename = os.path.basename(filepath)\n",
    "    sdkid_test = filename.split('_')[0]  # Assuming sdkid is the prefix before '_'\n",
    "    #print(sdkid_data)\n",
    "    if sdkid_test in test_ids:\n",
    "        test_data.append((filepath, label))\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, os.path.basename(img_path)\n",
    "    \n",
    "    \n",
    "\n",
    "# Define a variety of augmentations without resizing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    #transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize images \n",
    "])\n",
    "\n",
    "# Create the custom dataset instances for training, validation, and testing\n",
    "train_dataset = CustomDataset(data=train_data, transform=transform)\n",
    "valid_dataset = CustomDataset(data=valid_data, transform=transform)\n",
    "test_dataset = CustomDataset(data=test_data, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5e5dd-83e5-4a33-80af-455bfb61e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(img):\n",
    "    img = img * 0.5 + 0.5  # reverse normalization\n",
    "    return img\n",
    "\n",
    "# select 16 random indices for plotting\n",
    "indices = random.sample(range(len(train_dataset)), 16)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    image, label, image_name = train_dataset[idx]\n",
    "\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.numpy()\n",
    "\n",
    "    if image.shape[0] == 3:  # RGB\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        image = unnormalize(image)\n",
    "    elif image.shape[0] == 1:  # Grayscale\n",
    "        image = image.squeeze(0)\n",
    "        image = unnormalize(image)\n",
    "\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(image, cmap='gray' if image.ndim == 2 else None)\n",
    "    plt.title(f\"Label: {label}\", fontsize=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41153e-15a7-4cf3-913b-1dfb8f81460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, criterion, optimizer\n",
    "pc_model = densenet(pretrained=True).to(device)  # Single output for binary classification\n",
    "pc_model.fc = nn.Linear(pc_model.features.conv0, 1)\n",
    "pc_model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(pc_model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155ed00-660d-4b49-8e4a-fd47d1b2fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    n_total_steps = len(dataloader.sampler)\n",
    "    for images, targets, image_names in dataloader:\n",
    "        images, targets = images.to(device), targets.to(device).float().view(-1, 1) \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predictions = (torch.sigmoid(outputs) > 0.65).float()\n",
    "        train_correct += (predictions == targets).sum().item()\n",
    "            \n",
    "    # Calculate training accuracy\n",
    "    train_acc = train_correct / n_total_steps * 100\n",
    "    return train_acc\n",
    "\n",
    "def val(model, device, dataloader, criterion):\n",
    "    n_total_samples = 0\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_names in dataloader:\n",
    "            images, targets = images.to(device), targets.to(device).float().view(-1, 1) \n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predictions = (torch.sigmoid(outputs) > 0.65).float()\n",
    "            val_correct += (predictions == targets).sum().item()\n",
    "            n_total_samples += targets.size(0)\n",
    "        \n",
    "    val_acc = val_correct / n_total_samples * 100\n",
    "    avg_val_loss = val_loss / len(dataloader)\n",
    "    return avg_val_loss, val_acc\n",
    "\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    train_acc = train(pc_model, device, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = val(pc_model, device, val_loader, criterion)\n",
    "    print(f'Epoch [{e+1}/{n_epochs}], Train Accuracy: {train_acc:.2f}%, Val Accuracy: {val_acc:.2f}%, Val Loss: {val_loss:.4f}')\n",
    "       \n",
    "\n",
    "test_loss, test_acc = val(pc_model, device, test_loader, criterion)\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30e0d6-dde1-4f52-bf88-b592239d11b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
