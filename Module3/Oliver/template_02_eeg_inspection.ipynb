{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccbedda",
   "metadata": {},
   "source": [
    "# Experimentation 2: Inspecting our Data Set\n",
    "We will investigate and get a feel for EEG data.\n",
    "The focus lies on the aspect of accessing the file format,\n",
    "the contained meta and use data, and displaying it in a useful way.\n",
    "\n",
    "These tasks will help you consolidating your Python skills.\n",
    "We will also re-visit the general ideas presented in **Experimentation 1**.\n",
    "And -- having these preparations in place,\n",
    "we will be able to perform an effective start with the actual contents of the course.\n",
    "\n",
    "I will prepare an accompanying pdf document with additional information.\n",
    "\n",
    "\n",
    "## Reporting\n",
    "Not every plot or number generated in this template comes into the report.\n",
    "Nonetheless, some intermediary steps are required to achieve the relevant results.\n",
    "Cells, which have a *direct* relation to a part of the report, \n",
    "are marked with **Report:**.\n",
    "Be sure to not jump over relevant precursor steps.\n",
    "But when having little time, think about your priorities, and try to avoid spending time on decorative steps or cells which are not needed for the report.\n",
    "\n",
    "The relevant information about what to include in the report is the corresponding pdf document on itslearning.\n",
    "Annotations in this file are just for orientation and might not be complete.\n",
    "\n",
    "\n",
    "## What to do\n",
    "In this notebook, you will\n",
    "- load the data set\n",
    "- optional: downsample for plotting<br/>\n",
    "This step may not be that relevant when working locally, but could still improve navigation / zoom in the whole time series a lot, depending on your local resources (CPU/RAM).\n",
    "Note, even if you experience problems plotting the whole time series, you can still use full-detail plots on smaller regions.\n",
    "Advanced users could create an interactive zoom function, which will resample the data based on the number of visible points.\n",
    "- filter the data\n",
    "- investigate the autocorrelation function<br/>\n",
    "Note: Using pandas' autocorrelation_plot can be rather time consuming for a signal that size.\n",
    "A faster way is to use [scipy.signal's correlate](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html) to calculate the autocorrelation function and plot the result directly. \n",
    "- extract and display the seizure annotations\n",
    "- determine the stationarity of the time series\n",
    "\n",
    "## Discussion\n",
    "Prepare for discussion in class, which will take place either in groups or with your neighbours\n",
    "- What is the meaning of the filter coefficients?\n",
    "- What can you observe on the edges in the filter response?\n",
    "- Please compare to and discuss how the autocorrelation function of the selected EEG data relates to other data sets.\n",
    "- Compare the filterted signal to the original signal.\n",
    "- Is the available EEG data stationary? What do you observe?\n",
    "\n",
    "Feel free to use the forum on itslearning.\n",
    "## Good to Know\n",
    "### Note on using Cells in Notebooks\n",
    "When using Jupyter Notebooks, you can split your script into cells, which correspond to blocks of code which can be re-executed individually. Keep in mind though that the variables are shared, i.e., they are global variables.\n",
    "\n",
    "At the same time, you will find that you will modify parts of your notebook several times before you go on to the next step.\n",
    "\n",
    "Therefore, we recommend to not overwrite your variables when processing the data, otherwise, re-execution will lead to an iterated application of your algorithm, which will in most cases not have the desired effect.\n",
    "\n",
    "Example: (Good)\n",
    "```\n",
    "    # Cell 1:\n",
    "    raw_data = load_from(file)\n",
    "\n",
    "    # Cell 2:\n",
    "    filtered_data = filter(raw_data)\n",
    "```\n",
    "\n",
    "Imagine now, that Cell 2 would read ```raw_data = filter(raw_data)```.\n",
    "What would happen when re-executing Cell 2 several times?\n",
    "\n",
    "### Storing away pre-processed data\n",
    "Please note, there are many cases where it makes sense to write pre-processed data or extracted features to disc and start processing it with a new notebook/script. Whenever the pre-processing or feature extraction takes ages, and especially while developing the analysis (when redoing parts of the notebook again and again), you will want to store it away to not always having to re-run everything.\n",
    "\n",
    "Pickle can be a simple solution for storing and then loading your temporary data.\n",
    "\n",
    "https://docs.python.org/3/library/pickle.html\n",
    "https://wiki.python.org/moin/UsingPickle\n",
    "\n",
    "Short example for variable x:\n",
    "\n",
    "```\n",
    "import pickle\n",
    "\n",
    "# storing x\n",
    "pickle.dump(x, open('x-store.dat', 'wb')\n",
    "\n",
    "# loading x\n",
    "x = pickle.load(open('x-store.dat', 'rb')\n",
    "```\n",
    "\n",
    "Note: Pickle has same caveats, please search the internet for a thorough discussion.\n",
    "From an accessibility-perspective, using a general format like hdf5 or arrow might be preferrable.\n",
    "Domain and application specific formats like edf typically cannot hold your preprocessed data.\n",
    "But when working with Python, pickle is a fast and easy to use solution,\n",
    "but be aware that you shouldn't load pickled data from untrusted sources,\n",
    "as it could contain malicious code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                            # Array library\n",
    "\n",
    "import scipy                                  # Algorithms working on arrays\n",
    "\n",
    "\n",
    "# Jupyter lab supports interactive plots      # Matplotlib for plotting\n",
    "# using \"widget\"\n",
    "%matplotlib widget\n",
    "\n",
    "# Jupyter lab doesn't support notebook,\n",
    "# which was the preferred method for jupyter notebooks.\n",
    "#%matplotlib notebook\n",
    "#%matplotlib inline\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import seaborn as sns                         # Advanced plotting, support for data frames\n",
    "\n",
    "import pandas as pd                           # Advanced data frames & csv reading\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "# Adjust plot size & resolution for inline display.\n",
    "# Tune to your needs.\n",
    "plt.rcParams['figure.figsize'] = [9, 5.56]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Augmented Dickey-Fuller test\n",
    "from statsmodels.tsa.stattools import adfuller, pacf, acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d627313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining base paths for read-only and read-write data\n",
    "# will make it easy for us to switch between cloud\n",
    "# and local environments by just adjusting the paths.\n",
    "#\n",
    "# Also, it will prevent accidental overwriting of read-only data.\n",
    "\n",
    "from pathlib import Path         # OS agnostic path handling (/ vs \\)\n",
    "\n",
    "user = 'jb'                      # Per-user output directories\n",
    "\n",
    "# Base directories\n",
    "# DATA_DIR -- where the read-only sources are\n",
    "DATA_DIR = Path('/work/data')\n",
    "\n",
    "# OUTPUT_DIR -- where we will keep our data (read/write)\n",
    "# We will make sure it exists!\n",
    "OUTPUT_DIR = Path('/work/output')\n",
    "\n",
    "# Now create our own output directory and change to it\n",
    "OUTPUT_DIR /= user\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import os\n",
    "os.chdir(OUTPUT_DIR)             # Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7f1e4",
   "metadata": {},
   "source": [
    "# Data Access\n",
    "Now, we read the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2670942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyedflib import highlevel\n",
    "\n",
    "fn = DATA_DIR / \\\n",
    "    'module_3_-_time_series_analysis_for_eeg/seizure_eeg_data' / \\\n",
    "    '01.edf'\n",
    "print('Reading EEG from: {}'.format(fn))\n",
    "\n",
    "signals, signal_headers, header = \\\n",
    "    highlevel.read_edf(str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for all channels in the edf\n",
    "for i, sh in enumerate(signal_headers):\n",
    "    print('Channel {:2d}: {}'.format(i, sh['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4091e",
   "metadata": {},
   "source": [
    "# Inspecting the data\n",
    "Plot the data with ```plt.plot()```, use ```plt.xlabel()``` ```plt.ylabel()``` and ```plt.title()``` to provide names and units for the axes as well as the plot.\n",
    "Make sure the right units are displayed, i.e., time in seconds, and use the appropriate physical unit for the y axis.\n",
    "When you experience problems with display speed, use simple downsampling to achieve faster zooming at low zoom levels.\n",
    "\n",
    "Inspect a single channel in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372ff46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "197fb441",
   "metadata": {},
   "source": [
    "# Extracting Annotations\n",
    "To be able to create a model, we will need to when seizures happend. Thus, extract the seizure annotations and then use matplotlib patches to indicate seizure areas.\n",
    "\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Rectangle.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new plotting canvas for the downsampled plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Downsampled Signal with Annotations')\n",
    "plt.plot(plot_times, plot_channel)\n",
    "\n",
    "r = patches.Rectangle(\n",
    "    (1000, -1000), 4000, 3000,\n",
    "    linewidth=2, edgecolor='r', fill=False, zorder=100\n",
    ")\n",
    "ax.add_patch(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d5bd7",
   "metadata": {},
   "source": [
    "## Inspect all channels with seizure annotations\n",
    "Are there channels where you can see clear effects of the seizure?\n",
    "Try zooming.\n",
    "\n",
    "**Report:** Parts 3.1, 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0090e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4bc123a",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "What ever needs to be done!\n",
    "\n",
    "Please note, there are many cases where it makes sense to write pre-processed data or extracted features to disc and start processing it with a new notebook/script. Whenever the pre-processing or feature extraction takes ages, and especially while developing the analysis, you will want to store it away to not always having to re-run everything.\n",
    "\n",
    "Another solution is to check in one notebook if a certain result has been stored and then load that automatically.\n",
    "\n",
    "Pickle can be a simple solution for storing and then loading your temporary data.\n",
    "\n",
    "https://docs.python.org/3/library/pickle.html\n",
    "https://wiki.python.org/moin/UsingPickle\n",
    "\n",
    "Short example for variable x:\n",
    "\n",
    "```\n",
    "import pickle\n",
    "\n",
    "# storing x\n",
    "pickle.dump(x, open('x-store.dat', 'wb')\n",
    "\n",
    "# loading x\n",
    "x = pickle.load(open('x-store.dat', 'rb')\n",
    "```\n",
    "\n",
    "## Here: Create a band-pass filter based on a FIR filter\n",
    "Besides the desired EEG-signal, recording brain activity, our recording equipment picks up a lot of other stuff.\n",
    "This includes electronic noise (in the cuircuits), electrode movement, general muscle activity (from body movements),\n",
    "eye movements (the eyes are electrically charged!).\n",
    "To remove the noise, we apply a band-pass filter, which will cut of very slow and very fast changing components.\n",
    "\n",
    "Using the ```firwin``` function, we can generate a filter based on the given parameters,\n",
    "and using the ```convolve``` function, we can apply the filter to a signal.\n",
    "The possible parameters include the allowed frequencise, i.e., slowest and fastest allowed changes, as well as the filter length.\n",
    "The latter describes how many values the filter takes into account for calculating a value.\n",
    "\n",
    "Try different filter lengths. Start with 1 Hz as lower and 70 Hz as higher limit for the frequency.\n",
    "\n",
    "Plot the filter coefficients (the resulting array).\n",
    "\n",
    "```\n",
    "coefficients = scipy.signal.firwin(length of filter, [limits], sampling frequency, pass_zero=False)\n",
    "filtered_signal = scipy.signal.convolve(signal, coefficients, mode='same')\n",
    "```\n",
    "\n",
    "**References:**\n",
    "  * [The scipy function reference](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html)\n",
    "  * [The scipy cookbook](https://scipy-cookbook.readthedocs.io/items/FIRFilter.html)\n",
    "  * [Wikipedia on finite impulse response filters](https://en.wikipedia.org/wiki/Finite_impulse_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602764b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f5a2db8",
   "metadata": {},
   "source": [
    "## Filter response\n",
    "Use scipy.signal.freqz to get the filter response.\n",
    "Use insets to magnify the edges of the band pass.\n",
    "Use ```plt.xlim()``` and ```plt.ylim()``` to zoom in on the relevant parts.\n",
    "\n",
    "Check https://scipy-cookbook.readthedocs.io/items/FIRFilter.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter response\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Filter response')\n",
    "w, h = signal.freqz(_filter, worN=8192)\n",
    "plt.plot((w/np.pi)*nyq_freq, np.absolute(h), linewidth=2)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Gain')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.grid(True)\n",
    "\n",
    "# Upper inset plot.\n",
    "ax1 = plt.axes([0.42, 0.6, .45, .25])\n",
    "plt.plot((w/np.pi)*nyq_freq, np.absolute(h), linewidth=2)\n",
    "plt.grid(True)\n",
    "\n",
    "# Lower inset plot\n",
    "ax2 = plt.axes([0.42, 0.25, .45, .25])\n",
    "plt.plot((w/np.pi)*nyq_freq, np.absolute(h), linewidth=2)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b4173",
   "metadata": {},
   "source": [
    "## Compare the Filtered Signal to the Original Signal\n",
    "Plot both signals on top of each other.\n",
    "How do the filter response and the filtered signal connect?\n",
    "We will revisit this angle when doing Fourier decompositions in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fc6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ad210f4",
   "metadata": {},
   "source": [
    "# Autocorrelation\n",
    "\n",
    "Using pandas' autocorrelation_plot can be rather time consuming for a signal that size.\n",
    "\n",
    "A faster way is to use scipy.signal's correlate to calculate the autocorrelation function and plot the result directly. \n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html\n",
    "\n",
    "**Report:** Part 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_corr = signal.correlate(filtered, filtered)\n",
    "corr_lags = signal.correlation_lags(len(filtered), len(filtered))\n",
    "plt.figure()\n",
    "plt.plot(corr_lags, auto_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "#autocorrelation_plot(filtered)\n",
    "#plt.ylim((-0.2, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed24884",
   "metadata": {},
   "source": [
    "# Determine Stationarity\n",
    "Can you determine the stationarity for the full time series?\n",
    "The test actually requires a lot of memory to run, which especially laptops often do not have.\n",
    "In that case, work on a subset of the channel.\n",
    "\n",
    "And even if you can determine the stationarity for the whole time series,\n",
    "the question is, does the result of the test depend on down sampling or taking a part of the time series?\n",
    "So please try different amounts of data points.\n",
    "\n",
    "**Report:** Part 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0169c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdb10020-ce27-4f5f-9edb-6e3f7eb59699",
   "metadata": {},
   "source": [
    "# Convert to Meaningful Units\n",
    "The data files contain binary values recorded directly by analogue to digital converters.\n",
    "While we, of course, can just plot those values, the results will be hard to interpret.\n",
    "\n",
    "Luckily, it is possible to use the metadata for the channels to convert the unit less numbers to the actual measured quantities in Volts.\n",
    "To this end, the metadata contains the minimum and maximum values with the corresponding physical minimum and maximum values.\n",
    "\n",
    "Apply the correct transformation to get correct physical units and use them in your axes labels when plotting the signal.\n",
    "\n",
    "**Short on time?** Jump over this meaningful units block for now and focus on the other tasks first. Having useful units is desirable, but when pressed, the priority should be on completing the detector.\n",
    "\n",
    "**Report:** Part 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485efd31-9465-47c1-abe9-e51b1db34fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef53098c-ea3d-41dd-8bed-0783f864b7ac",
   "metadata": {},
   "source": [
    "# The (Optional) Final Step\n",
    "\n",
    "Dump your preprocessed data into a file, for example with pickle, so that you can reuse it in the detection template instead of copying over all the code.\n",
    "\n",
    "For template 03, we will need all channels in filtered form.\n",
    "Instead of copying and adjusting the code from this template,\n",
    "you can also filter all channels here and pickle them away for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c85c5b-1f77-4009-9d65-dba9c4e6a930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
